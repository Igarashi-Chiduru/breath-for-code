---
title: 文本和字节序列
order: 3
---

# 文本和字节序列

<Alert type="info">`Python3` 明确区分了人类可读的**文本字符串**和原始的**字节序列**</Alert>

## 1.字符

### 1.1 什么是字符串

**“字符串”** 是个相当简单的概念：一个字符串是一个字符序列。问题出现在 “字符” 的定义上。

> 2015 年: "字符" 的最佳定义为 Unicode 字符。因此 `Python3` 的 `str` 对象获取的元素就是 Unicode 字符。

Unicode 标准把 **字符的标识** 和具体的 **字节表述** 进行了如下区分：

- **字符的标识**：即 `码位` ，是 0 ~ 1 114 111 的数字 (十进制)，在 Unicode 标准中以 4 ~ 6 个十六进制数字表示，而且加前缀 `U+`。

  - > 如：A 的码位 为 U+0041

- **字节表述**：取决于所用的编码，在 `UTF-8` 编码中，A(U+0041) 的码位编码为但字节的 `\x41`，而在 `UTF-16LE` 编码中编码成两个字节 `\x41\x00`

  - 编码：在 `码位` 和 `字节序列` 之间转换时使用的算法

<Alert type="warning">把`码位` 转换成 `字节序列` 的过程是**编码(encode)**，把 `字节序列` 转化为码位的过程是**解码(decode)**</Alert>

```python
s = "coffeé"
print(len(s))

b_sequence = s.encode("utf8")
print(b_sequence, len(b_sequence))

print(b_sequence.decode("utf8"))

### 输出结果
# 6
# b'coffe\xc3\xa9' 7
# coffeé
```

- 字符串 `coffeé` 有 6 个 Unicode 字符
- 使用 `UTF-8` 把 `str` 对象 编码为 `bytes` 对象，该对象以 `b` 开头
- 字节序列 `b_sequence` 有 7 个字节（在 `UTF-8` 中 `é` 的码位编码成 2 个字节）
- 使用 `bytes.decode("utf-8")` 将 `bytes` 对象解码为 `str` 对象

### 1.2 编码的发展史

##### **1.2.1 ASCII 码**

用 8 位，即 **一个字节** 表示字符 0 ~ 127 已编好的英文 和 拉丁字符

> 后为了扩容，各国根据索引等方式又进行进一步的编码
>
> 中国：
>
> - `gb2312`: 只收录了约 6700 多个中文 (1980 年)
> - `gbk1.0`: 收录了 2 万 多个字符 (1995 年)
> - `gb18030`：27000 左右中文 (2000 年)

##### **1.2.2 Unicode 万国码**

**Unicode**: 兼容各国的编码，是一种国际通用标准， utf-8 则是 Unicode 的子集

- `utf-32`: 即一个字符占 32 位 4 字节

- `utf-16`: 一个字符占 2 字节 及以上，65535

- `utf-8`: Unicode 的子集，英文用 ASCII 码 来存储

  > 亚洲国家如: 中 / 日 用 3 个字节

  编码的转换：
  过程：
  utf-8--->decode---->unicode
  unicode--->encode----->gbk
  python2:
  2 的默认编码为 ASCII 码 因此通常要在文件头声明用什么方式编码
  格式:#-_- coding:utf-8 -_-
  .decode("utf-8") 解码要先声明是什么来解码 里是 utf-8 因此声明为 utf-8 编码（同 str(bytes,encoding="")内置函数）
  .encode("gbk") 这个是编码，把字符串编码成 16 进制的 bytes 类型（同 bytes(str,encoding="")内置函数）

        python3:
            3不同2 默认编码在官方给的声明上是Unicode 这就能解释了为什么在gbk编码的cmd下能够正常显示字符了
            但是利用impot sys下的import sys  .getdefaultencoding()则显示默认文件编码为utf-8 这里待论
            .encode("gbk")
            过程：编码的过程实质上是先解码在编码，若没有指定解码方式则用python默认的方式解码
            这里要额外说的是另一种类型了
            bytes 类型:
                b = bytes = 字节类型 一个个[0-255]的数字(16进制)，每个国家都有自己的bytes类型，通用的则是utf-8
            这里运用encode("gbk")已经返回的bytes类型的码，因此中文不可见英文可见 把数据转为了bytes（这里bytes是对应各种编码的16进制）
            利用.decode("gbk")便可以正常打印，但没有任何意义，相当于又解码回去了原来的Unicode 把bytes转为字符串
            （相对于转为对应编码的16进制（这么做主要就是用于传输数据等等），再转为万国码的字符串）

        编码意义:大概是在于数据的传输交互吧

又论 2、3 编码：
例如 py2 中声明可以要加#coding:utf8(若有处理英文这是必须的)

    在python2中：字符串其实存的不是字符串，而是对应的字节
    比如 py2的str = "你好hello" 其中 type(str) <str>  len(str) = 11  repr(str) = 存的是十六进制的对应字节码和英文字符hello
        而通常转Unicode只需要在前面加一个u，此时type中即是unicode数据类型，内容则是对应的Unicode
    因此：（python2的最大特点）但凡打印时能看到明文的（也就是有汉字的）都是Unicode数据类型， print方法进行了转换，并且2的编码可以拼接
        如：
            print "你好"+u"再见"这样把两个不同编码的字符串拼接成Unicode类型的数据，但在处理非ascii码的数据会直接报错

    而在py3 中 str = "你好hello"  type(str) <str> len(str) = 5 repr(str) = "你好hello"这样的字符串，就是因为py3中的编码默认Unicode
    decode的数据是密文，而print后的数据则是明文
    若想看存的编码在py2中可以用repr，可是若py3中则用json.dump(s)查看

接下来讨论一个问题：
在 py2 中为什么加了 coding:uft-8 就不会报错 1.声明给解释器看，py2 的默认编码方式是 ASCII 码 用 sys.getdefaultencoding()来查看系统当前编码,3 是 utf8
因此要知道解释器是用什么解释的和代码是用什么保存的（因为就是说解释器和磁盘存盘的编码一致才能顺利执行）
解释器：打开文件，一行行的代码放到内存

    2. py文件存储的的确是utf8的编码，而在内存中生成的时候就会自动变成unicode编码！！！

    3.cmd下中文字符为什么py2会报错而py3不会：
        因为py2的解释器默认是用ascii来编码，而py2 用coding:utf8告诉解释器用utf8编码文件，而py2文件存储也是用utf8来存储，按理来说不应该会有乱码
        这就是因为cmd也是软件（可执行程序）py2在cmd下显示传给cmd的是ascii的编码，而cmd的默认解码方式跟随系统（即utf8）故出错。
        而py3传给cmd的默认是unicode编码，符合ISO统一标准，所以没问题。
